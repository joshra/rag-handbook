# 02-線上評估：A/B / 回饋閉環 / 人工標註流程

## 你將學到（Learning Objectives）

- 能設計線上評估：不只看點擊，還要看「可用性與信任」
- 能建立回饋閉環：使用者回饋 → 標註 → 改索引/改提示 → 回歸
- 能把線上與離線指標接起來（避免各說各話）

## 本章地圖

- **適合用在**：要把線上流量變成可用回饋，支援 A/B 與人工標註時。
- **你會做出**：回饋收集、抽樣標註、A/B 與回歸題庫回補流程。
- **最可能踩雷**：只收「讚/倒讚」不收理由；不記版本導致無法歸因。

## 線上要量測什麼

除了基本的延遲與錯誤率，你還需要：

- **引用點擊率/展開率**：使用者是否信任證據
- **追問比例**：問題是否常缺上下文（可回饋到 UI 或 query 改寫）
- **拒答比例**：是否過度保守或過度亂答
- **負回饋率**：thumb down / 回報錯誤

## A/B 的最小做法

你可以把變更分成幾類做 A/B：

- chunking 版本（chunker_version）
- embedding 模型版本（embedding_model_version）
- retriever 參數（top-k/MMR/hybrid）
- prompt 版本（prompt_version）
- rerank 開關/模型

原則：
- 一次只測一個主要變因（不然不知道是誰造成差異）

## 回饋閉環（工程流程）

1. 線上收集回饋（正負評、引用是否有用、是否答非所問）
2. 抽樣進標註池（建立標註任務）
3. 標註：正確證據是什麼？答案是否正確？為什麼錯？
4. 匯總失敗分類（retrieval miss / noisy / generation）
5. 針對性改進（改切分/改 filters/加 rerank/改 prompt）
6. 離線回歸 → 小流量上線 → 全量

## 人工標註的最小規格（建議）

每筆至少標：

- 是否可回答（是/否/需澄清）
- 正確證據 chunk（可多選）
- 答案是否正確（是/否/部分）
- 錯誤原因分類

## 本章小結

- 線上評估的關鍵是歸因：每次回答都要能對到資料/索引/模型/prompt 版本。
- 回饋閉環要可執行：收集 → 抽樣 → 標註 → 失敗分類 → 回補題庫 → 回歸。
- A/B 要先定義守門指標（越權/錯誤率/P95/成本），再談提升準確率。

## 延伸閱讀

- [03-可觀測性：log-trace-命中率-失敗分類](03-可觀測性：log-trace-命中率-失敗分類.md)
- [01-Checklist（上線前-事故後）](../10-附錄/01-Checklist（上線前-事故後）.md)

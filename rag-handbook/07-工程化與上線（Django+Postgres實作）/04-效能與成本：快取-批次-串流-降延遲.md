# 04-效能與成本：快取 / 批次 / 串流 / 降延遲

## 你將學到（Learning Objectives）

- 能用「分解延遲」定位瓶頸：檢索、rerank、LLM、序列化、網路
- 能用最小的工程手段把成本與延遲壓到可上線（P95 目標）
- 能避免常見成本黑洞：重複 embedding、無限制 top-k、過度長 context

## 本章地圖

- **適合用在**：P95 延遲或成本過高，需要優先把系統做穩時。
- **你會做出**：快取層級、批次策略、串流與降級的上線配方。
- **最可能踩雷**：只優化 LLM；忽略檢索/embedding/外部依賴的尾延遲。

## 先做觀測，再談優化

每次 query 建議至少記：

- retrieval latency、rerank latency、llm latency、total latency
- top-k 候選數、最終引用數
- token（prompt/completion）或 context 長度（近似）

沒有這些，後面任何優化都會變成「盲人摸象」。

## 成本/延遲的常見主因

- **候選數太大**：top-k 取太多 → rerank/packing/LLM 成本暴增
- **chunk 太大或太多**：context 超長 → LLM 變慢、也更不穩
- **沒有快取**：重複問題一直打 embedding/檢索/LLM
- **串流未使用**：使用者體感延遲高（其實總耗時不變）

## 實務優化順序（建議）

1. **限制 top-k 與候選數**
2. **加入檢索快取**（先快取 top-k chunks）
3. **加入 embedding 快取**（query embedding / chunk embedding 都可能受益）
4. **加入 rerank（候選數控制住後）**
5. **LLM 串流**（提升體感）
6. **批次**：embedding 批次、資料導入批次 upsert

## 降級策略（讓系統在壓力下仍可用）

- rerank 超時 → 直接用向量排序
- LLM 超時/失敗 → 回傳「證據不足/服務忙碌」並附上引用片段（可選）
- 向量庫延遲飆高 → 退回全文檢索或快取（視需求）

## 本章小結

- 先把耗時拆段量測（retrieval/rerank/LLM），再決定快取與降級放在哪。
- 檢索快取與 embedding 快取通常是最划算的優化；query 結果快取要小心權限與一致性。
- timeout、降級與串流要一起設計：穩定性比平均延遲更重要。

## 延伸閱讀

- [03-Django整合：API設計-背景任務-快取-速率限制](03-Django整合：API設計-背景任務-快取-速率限制.md)
- [03-Rerank（二階段檢索）與常見模型](../04-檢索策略（把找回來的內容變準）/03-Rerank（二階段檢索）與常見模型.md)

# 02-向量索引與相似度（cosine / dot / L2）

## 你將學到（Learning Objectives）

- 知道相似度在算什麼，以及它跟 embedding 模型的關係
- 知道「索引」在解決什麼問題（精確 vs 近似 ANN）
- 能在工程上做出正確選擇：先可用、再效能、最後成本

## 本章地圖

- **適合用在**：要理解相似度/索引型別，或查詢很慢、命中怪怪時。
- **你會做出**：相似度選擇與索引策略（含 pgvector 導入注意事項）。
- **最可能踩雷**：度量與正規化搞錯、索引未命中、top-k 設太大。

## 相似度是什麼

把 query 與 chunk 都變成向量後，你需要一個距離/相似度函數來排名。

常見三種：

- **cosine similarity**：看角度相近（常見）
- **dot product**：內積（常見，與向量是否正規化有關）
- **L2 distance**：歐式距離（有些模型/場景會用）

重點：你不一定要「自己挑」，因為很多 embedding 模型與向量庫會有建議搭配。
你要確保的是：**同一套資料與查詢使用一致的規則**，並且可量測。

## 索引在解什麼

資料量一大，你不能每次都把 query 跟所有向量逐一比對（太慢）。

所以會有索引/ANN：

- **精確搜尋**：最準，但慢（資料少或離線評估可用）
- **ANN（近似）**：用資料結構加速，速度快但可能略降 recall

pgvector 或專用向量庫的差異，很多時候就在於：
- ANN 能力成熟度
- 參數/分片/觀測/成本控制

## 你需要關心的工程面要點

- **embedding 維度固定**：維度一變，索引就得重建
- **向量正規化**：有些模型建議先 normalize（影響 cosine/dot）
- **查詢與資料一致**：同一個 embedding model（至少同一版本）
- **基準測試**：以真實 filters 測延遲與命中率（P50/P95 + recall@k）

## 本章小結

- cosine/dot/L2 的差異會影響排序；要跟 embedding 模型的輸出特性一起看。
- 索引不是萬靈丹：資料分佈、filter、k 值與查詢形狀會決定是否用得上索引。
- 先用小規模壓測驗證延遲與 recall，再決定要不要升級專用向量庫。

## 延伸閱讀

- [02-PostgreSQL+pgvector導入（schema-索引-查詢樣式）](../07-工程化與上線（Django+Postgres實作）/02-PostgreSQL+pgvector導入（schema-索引-查詢樣式）.md)
- [02-top-k-filter-MMR-多路召回](../04-檢索策略（把找回來的內容變準）/02-top-k-filter-MMR-多路召回.md)
